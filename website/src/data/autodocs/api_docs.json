{
  "package": {
    "name": "cogency",
    "version": "unknown",
    "docstring": ""
  },
  "modules": {
    "agent": {
      "name": "agent",
      "docstring": "",
      "classes": [
        {
          "name": "Agent",
          "docstring": "Magical 6-line DX that just works.\n\nArgs:\n    name: Agent identifier\n    llm: Language model instance  \n    tools: Optional list of tools for agent to use\n    trace: Enable execution tracing for debugging (default: True)",
          "module": "cogency.agent",
          "methods": [
            {
              "name": "run",
              "docstring": "",
              "signature": "(*args, **kwargs)"
            },
            {
              "name": "run_streaming",
              "docstring": "Run agent with beautiful streaming output to console - perfect for demos.",
              "signature": "(self, query: str, context: Optional[cogency.context.Context] = None, mode: Optional[Literal['summary', 'trace', 'dev', 'explain']] = None)"
            },
            {
              "name": "stream",
              "docstring": "",
              "signature": "(*args, **kwargs)"
            }
          ],
          "init_signature": "(self, name: str, llm: Optional[cogency.llm.base.BaseLLM] = None, tools: Optional[List[cogency.tools.base.BaseTool]] = None, trace: bool = True, memory_dir: str = '.memory', prompt_fragments: Optional[Dict[str, Dict[str, str]]] = None, default_output_mode: Literal['summary', 'trace', 'dev', 'explain'] = 'summary')"
        },
        {
          "name": "State",
          "docstring": "dict() -> new empty dictionary\ndict(mapping) -> new dictionary initialized from a mapping object's\n    (key, value) pairs\ndict(iterable) -> new dictionary initialized as if via:\n    d = {}\n    for k, v in iterable:\n        d[k] = v\ndict(**kwargs) -> new dictionary initialized with the name=value pairs\n    in the keyword argument list.  For example:  dict(one=1, two=2)",
          "module": "cogency.types",
          "methods": [],
          "init_signature": "(self, /, *args, **kwargs)"
        },
        {
          "name": "BaseLLM",
          "docstring": "Base class for all LLM implementations in the cogency framework.\n\nAll LLM providers support:\n- Streaming execution for real-time output\n- Key rotation for high-volume usage  \n- Rate limiting via yield_interval parameter\n- Unified interface across providers\n- Dynamic model/parameter configuration",
          "module": "cogency.llm.base",
          "methods": [
            {
              "name": "ainvoke",
              "docstring": "LangGraph compatibility method - wrapper around invoke().",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "invoke",
              "docstring": "Generate a response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    **kwargs: Additional parameters for the LLM call\n\nReturns:\n    String response from the LLM",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "stream",
              "docstring": "Generate a streaming response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    yield_interval: Minimum time between yields for rate limiting (seconds)\n    **kwargs: Additional parameters for the LLM call\n\nYields:\n    String chunks from the LLM response",
              "signature": "(self, messages: List[Dict[str, str]], yield_interval: float = 0.0, **kwargs) -> AsyncIterator[str]"
            }
          ],
          "init_signature": "(self, api_key: str = None, key_rotator=None, **kwargs)"
        },
        {
          "name": "BaseTool",
          "docstring": "Base class for all tools in the cogency framework.",
          "module": "cogency.tools.base",
          "methods": [
            {
              "name": "get_schema",
              "docstring": "Return tool call schema for LLM formatting.\n\nReturns:\n    String representation of the tool's parameter schema",
              "signature": "(self) -> str"
            },
            {
              "name": "get_usage_examples",
              "docstring": "Return example tool calls for LLM guidance.\n\nReturns:\n    List of example tool call strings",
              "signature": "(self) -> List[str]"
            },
            {
              "name": "run",
              "docstring": "Execute the tool with the given parameters.\n\nReturns:\n    Dict containing the tool's results or error information",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            },
            {
              "name": "validate_and_run",
              "docstring": "Validate parameters then run the tool.",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            }
          ],
          "init_signature": "(self, name: str, description: str)"
        },
        {
          "name": "CircuitOpenError",
          "docstring": "Circuit breaker is open.",
          "module": "cogency.core.resilience",
          "methods": [],
          "init_signature": "(self, /, *args, **kwargs)"
        },
        {
          "name": "Context",
          "docstring": "Agent operational context.",
          "module": "cogency.context",
          "methods": [
            {
              "name": "add_message",
              "docstring": "Add message to history with optional trace linkage.",
              "signature": "(self, role: str, content: str, trace_id: Optional[str] = None)"
            },
            {
              "name": "add_tool_result",
              "docstring": "Add tool execution result to history.",
              "signature": "(self, tool_name: str, args: dict, output: dict)"
            },
            {
              "name": "get_clean_conversation",
              "docstring": "Returns conversation without execution trace data and internal JSON.",
              "signature": "(self) -> List[Dict[str, str]]"
            }
          ],
          "init_signature": "(self, current_input: str, messages: List[Dict[str, str]] = None, tool_results: Optional[List[Dict[str, Any]]] = None, max_history: Optional[int] = None)"
        },
        {
          "name": "ExecutionTrace",
          "docstring": "Lean trace engine - just stores entries with serialization safety.",
          "module": "cogency.types",
          "methods": [
            {
              "name": "add",
              "docstring": "",
              "signature": "(self, node: str, message: str, data: dict = None, explanation: str = None)"
            }
          ],
          "init_signature": "(self)"
        },
        {
          "name": "FSMemory",
          "docstring": "Filesystem-based memory backend.\n\nStores memory artifacts as JSON files in a directory structure.\nUses simple text matching for recall operations.",
          "module": "cogency.memory.filesystem",
          "methods": [
            {
              "name": "clear",
              "docstring": "Remove all artifact files.",
              "signature": "(self) -> None"
            },
            {
              "name": "forget",
              "docstring": "Remove artifact file.",
              "signature": "(self, artifact_id: uuid.UUID) -> bool"
            },
            {
              "name": "inspect",
              "docstring": "Dev tooling - inspect memory state.",
              "signature": "(self) -> Dict[str, Any]"
            },
            {
              "name": "memorize",
              "docstring": "Store content as JSON file.",
              "signature": "(self, content: str, memory_type: cogency.memory.base.MemoryType = <MemoryType.FACT: 'fact'>, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, timeout_seconds: float = 10.0) -> cogency.memory.base.MemoryArtifact"
            },
            {
              "name": "recall",
              "docstring": "Search artifacts with enhanced relevance scoring and async optimization.",
              "signature": "(self, query: str, limit: Optional[int] = None, tags: Optional[List[str]] = None, memory_type: Optional[cogency.memory.base.MemoryType] = None, since: Optional[str] = None, **kwargs) -> List[cogency.memory.base.MemoryArtifact]"
            },
            {
              "name": "should_store",
              "docstring": "Smart auto-storage heuristics - NO BULLSHIT.",
              "signature": "(self, text: str) -> Tuple[bool, str]"
            }
          ],
          "init_signature": "(self, memory_dir: str = '.memory')"
        },
        {
          "name": "RateLimitedError",
          "docstring": "Request was rate limited.",
          "module": "cogency.core.resilience",
          "methods": [],
          "init_signature": "(self, /, *args, **kwargs)"
        },
        {
          "name": "ToolRegistry",
          "docstring": "Auto-discovery registry for tools.",
          "module": "cogency.tools.registry",
          "methods": [],
          "init_signature": "(self, /, *args, **kwargs)"
        },
        {
          "name": "Tracer",
          "docstring": "Handles formatting and output of execution traces.",
          "module": "cogency.core.tracer",
          "methods": [
            {
              "name": "output",
              "docstring": "Output trace based on mode.",
              "signature": "(self, mode: Literal['summary', 'trace', 'dev', 'explain'])"
            }
          ],
          "init_signature": "(self, trace: cogency.types.ExecutionTrace)"
        },
        {
          "name": "Workflow",
          "docstring": "Abstracts LangGraph complexity for magical Agent DX.",
          "module": "cogency.workflow",
          "methods": [],
          "init_signature": "(self, llm, tools, memory: cogency.memory.base.BaseMemory, routing_table: Optional[Dict] = None, prompt_fragments: Optional[Dict[str, Dict[str, str]]] = None)"
        }
      ],
      "functions": [
        {
          "name": "auto_detect_llm",
          "docstring": "Auto-detect LLM provider from environment variables.\n\nFallback chain:\n1. OpenAI\n2. Anthropic\n3. Gemini\n4. Grok\n5. Mistral\n\nReturns:\n    BaseLLM: Configured LLM instance\n    \nRaises:\n    RuntimeError: If no API keys found for any provider.",
          "module": "cogency.llm.auto",
          "signature": "() -> cogency.llm.base.BaseLLM"
        },
        {
          "name": "counter",
          "docstring": "Record counter metric.",
          "module": "cogency.core.metrics",
          "signature": "(name: str, value: float = 1.0, tags: Optional[Dict[str, str]] = None)"
        },
        {
          "name": "get_metrics",
          "docstring": "Get global metrics collector.",
          "module": "cogency.core.metrics",
          "signature": "() -> cogency.core.metrics.MetricsCollector"
        },
        {
          "name": "histogram",
          "docstring": "Record histogram metric.",
          "module": "cogency.core.metrics",
          "signature": "(name: str, value: float, tags: Optional[Dict[str, str]] = None)"
        },
        {
          "name": "with_metrics",
          "docstring": "Decorator to automatically time function execution.",
          "module": "cogency.core.metrics",
          "signature": "(metric_name: str, tags: Optional[Dict[str, str]] = None)"
        }
      ]
    },
    "llm": {
      "name": "llm",
      "docstring": "",
      "classes": [
        {
          "name": "AnthropicLLM",
          "docstring": "Base class for all LLM implementations in the cogency framework.\n\nAll LLM providers support:\n- Streaming execution for real-time output\n- Key rotation for high-volume usage  \n- Rate limiting via yield_interval parameter\n- Unified interface across providers\n- Dynamic model/parameter configuration",
          "module": "cogency.llm.anthropic",
          "methods": [
            {
              "name": "ainvoke",
              "docstring": "LangGraph compatibility method - wrapper around invoke().",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "invoke",
              "docstring": "Generate a response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    **kwargs: Additional parameters for the LLM call\n\nReturns:\n    String response from the LLM",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "stream",
              "docstring": "Generate a streaming response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    yield_interval: Minimum time between yields for rate limiting (seconds)\n    **kwargs: Additional parameters for the LLM call\n\nYields:\n    String chunks from the LLM response",
              "signature": "(self, messages: List[Dict[str, str]], yield_interval: float = 0.0, **kwargs) -> AsyncIterator[str]"
            }
          ],
          "init_signature": "(self, api_keys: Union[str, List[str]] = None, model: str = 'claude-3-5-sonnet-20241022', timeout: float = 15.0, temperature: float = 0.7, max_tokens: int = 4096, max_retries: int = 3, **kwargs)"
        },
        {
          "name": "BaseLLM",
          "docstring": "Base class for all LLM implementations in the cogency framework.\n\nAll LLM providers support:\n- Streaming execution for real-time output\n- Key rotation for high-volume usage  \n- Rate limiting via yield_interval parameter\n- Unified interface across providers\n- Dynamic model/parameter configuration",
          "module": "cogency.llm.base",
          "methods": [
            {
              "name": "ainvoke",
              "docstring": "LangGraph compatibility method - wrapper around invoke().",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "invoke",
              "docstring": "Generate a response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    **kwargs: Additional parameters for the LLM call\n\nReturns:\n    String response from the LLM",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "stream",
              "docstring": "Generate a streaming response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    yield_interval: Minimum time between yields for rate limiting (seconds)\n    **kwargs: Additional parameters for the LLM call\n\nYields:\n    String chunks from the LLM response",
              "signature": "(self, messages: List[Dict[str, str]], yield_interval: float = 0.0, **kwargs) -> AsyncIterator[str]"
            }
          ],
          "init_signature": "(self, api_key: str = None, key_rotator=None, **kwargs)"
        },
        {
          "name": "GeminiLLM",
          "docstring": "Base class for all LLM implementations in the cogency framework.\n\nAll LLM providers support:\n- Streaming execution for real-time output\n- Key rotation for high-volume usage  \n- Rate limiting via yield_interval parameter\n- Unified interface across providers\n- Dynamic model/parameter configuration",
          "module": "cogency.llm.gemini",
          "methods": [
            {
              "name": "ainvoke",
              "docstring": "LangGraph compatibility method - wrapper around invoke().",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "invoke",
              "docstring": "Generate a response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    **kwargs: Additional parameters for the LLM call\n\nReturns:\n    String response from the LLM",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "stream",
              "docstring": "Generate a streaming response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    yield_interval: Minimum time between yields for rate limiting (seconds)\n    **kwargs: Additional parameters for the LLM call\n\nYields:\n    String chunks from the LLM response",
              "signature": "(self, messages: List[Dict[str, str]], yield_interval: float = 0.0, **kwargs) -> AsyncIterator[str]"
            }
          ],
          "init_signature": "(self, api_keys: Union[str, List[str]] = None, model: str = 'gemini-2.5-flash', timeout: float = 15.0, temperature: float = 0.7, max_retries: int = 3, **kwargs)"
        },
        {
          "name": "GrokLLM",
          "docstring": "Base class for all LLM implementations in the cogency framework.\n\nAll LLM providers support:\n- Streaming execution for real-time output\n- Key rotation for high-volume usage  \n- Rate limiting via yield_interval parameter\n- Unified interface across providers\n- Dynamic model/parameter configuration",
          "module": "cogency.llm.grok",
          "methods": [
            {
              "name": "ainvoke",
              "docstring": "LangGraph compatibility method - wrapper around invoke().",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "invoke",
              "docstring": "Generate a response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    **kwargs: Additional parameters for the LLM call\n\nReturns:\n    String response from the LLM",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "stream",
              "docstring": "Generate a streaming response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    yield_interval: Minimum time between yields for rate limiting (seconds)\n    **kwargs: Additional parameters for the LLM call\n\nYields:\n    String chunks from the LLM response",
              "signature": "(self, messages: List[Dict[str, str]], yield_interval: float = 0.0, **kwargs) -> AsyncIterator[str]"
            }
          ],
          "init_signature": "(self, api_keys: Union[str, List[str]] = None, model: str = 'grok-beta', timeout: float = 15.0, temperature: float = 0.7, max_retries: int = 3, **kwargs)"
        },
        {
          "name": "KeyRotator",
          "docstring": "Simple key rotator for API rate limit avoidance.",
          "module": "cogency.llm.key_rotator",
          "methods": [
            {
              "name": "get_key",
              "docstring": "Get next key in rotation.",
              "signature": "(self) -> str"
            },
            {
              "name": "rotate_key",
              "docstring": "Rotate to next key immediately. Returns feedback.",
              "signature": "(self) -> str"
            }
          ],
          "init_signature": "(self, keys: List[str])"
        },
        {
          "name": "MistralLLM",
          "docstring": "Base class for all LLM implementations in the cogency framework.\n\nAll LLM providers support:\n- Streaming execution for real-time output\n- Key rotation for high-volume usage  \n- Rate limiting via yield_interval parameter\n- Unified interface across providers\n- Dynamic model/parameter configuration",
          "module": "cogency.llm.mistral",
          "methods": [
            {
              "name": "ainvoke",
              "docstring": "LangGraph compatibility method - wrapper around invoke().",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "invoke",
              "docstring": "Generate a response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    **kwargs: Additional parameters for the LLM call\n\nReturns:\n    String response from the LLM",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "stream",
              "docstring": "Generate a streaming response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    yield_interval: Minimum time between yields for rate limiting (seconds)\n    **kwargs: Additional parameters for the LLM call\n\nYields:\n    String chunks from the LLM response",
              "signature": "(self, messages: List[Dict[str, str]], yield_interval: float = 0.0, **kwargs) -> AsyncIterator[str]"
            }
          ],
          "init_signature": "(self, api_keys: Union[str, List[str]] = None, model: str = 'mistral-large-latest', timeout: float = 15.0, temperature: float = 0.7, max_tokens: int = 4096, max_retries: int = 3, **kwargs)"
        },
        {
          "name": "OpenAILLM",
          "docstring": "Base class for all LLM implementations in the cogency framework.\n\nAll LLM providers support:\n- Streaming execution for real-time output\n- Key rotation for high-volume usage  \n- Rate limiting via yield_interval parameter\n- Unified interface across providers\n- Dynamic model/parameter configuration",
          "module": "cogency.llm.openai",
          "methods": [
            {
              "name": "ainvoke",
              "docstring": "LangGraph compatibility method - wrapper around invoke().",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "invoke",
              "docstring": "Generate a response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    **kwargs: Additional parameters for the LLM call\n\nReturns:\n    String response from the LLM",
              "signature": "(self, messages: List[Dict[str, str]], **kwargs) -> str"
            },
            {
              "name": "stream",
              "docstring": "Generate a streaming response from the LLM given a list of messages.\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content' keys\n    yield_interval: Minimum time between yields for rate limiting (seconds)\n    **kwargs: Additional parameters for the LLM call\n\nYields:\n    String chunks from the LLM response",
              "signature": "(self, messages: List[Dict[str, str]], yield_interval: float = 0.0, **kwargs) -> AsyncIterator[str]"
            }
          ],
          "init_signature": "(self, api_keys: Union[str, List[str]] = None, model: str = 'gpt-4o', timeout: float = 15.0, temperature: float = 0.7, max_retries: int = 3, **kwargs)"
        }
      ],
      "functions": [
        {
          "name": "auto_detect_llm",
          "docstring": "Auto-detect LLM provider from environment variables.\n\nFallback chain:\n1. OpenAI\n2. Anthropic\n3. Gemini\n4. Grok\n5. Mistral\n\nReturns:\n    BaseLLM: Configured LLM instance\n    \nRaises:\n    RuntimeError: If no API keys found for any provider.",
          "module": "cogency.llm.auto",
          "signature": "() -> cogency.llm.base.BaseLLM"
        }
      ]
    },
    "tools": {
      "name": "tools",
      "docstring": "",
      "classes": [
        {
          "name": "BaseTool",
          "docstring": "Base class for all tools in the cogency framework.",
          "module": "cogency.tools.base",
          "methods": [
            {
              "name": "get_schema",
              "docstring": "Return tool call schema for LLM formatting.\n\nReturns:\n    String representation of the tool's parameter schema",
              "signature": "(self) -> str"
            },
            {
              "name": "get_usage_examples",
              "docstring": "Return example tool calls for LLM guidance.\n\nReturns:\n    List of example tool call strings",
              "signature": "(self) -> List[str]"
            },
            {
              "name": "run",
              "docstring": "Execute the tool with the given parameters.\n\nReturns:\n    Dict containing the tool's results or error information",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            },
            {
              "name": "validate_and_run",
              "docstring": "Validate parameters then run the tool.",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            }
          ],
          "init_signature": "(self, name: str, description: str)"
        },
        {
          "name": "CalculatorTool",
          "docstring": "Base class for all tools in the cogency framework.",
          "module": "cogency.tools.calculator",
          "methods": [
            {
              "name": "get_schema",
              "docstring": "Return tool call schema for LLM formatting.\n\nReturns:\n    String representation of the tool's parameter schema",
              "signature": "(self) -> str"
            },
            {
              "name": "get_usage_examples",
              "docstring": "Return example tool calls for LLM guidance.\n\nReturns:\n    List of example tool call strings",
              "signature": "(self) -> List[str]"
            },
            {
              "name": "run",
              "docstring": "Perform calculator operations.",
              "signature": "(self, operation: str, x1: float = None, x2: float = None) -> Dict[str, Any]"
            },
            {
              "name": "validate_and_run",
              "docstring": "Validate parameters then run the tool.",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            }
          ],
          "init_signature": "(self)"
        },
        {
          "name": "FileManagerTool",
          "docstring": "File operations within a safe base directory.",
          "module": "cogency.tools.file_manager",
          "methods": [
            {
              "name": "get_schema",
              "docstring": "Return tool call schema for LLM formatting.\n\nReturns:\n    String representation of the tool's parameter schema",
              "signature": "(self) -> str"
            },
            {
              "name": "get_usage_examples",
              "docstring": "Return example tool calls for LLM guidance.\n\nReturns:\n    List of example tool call strings",
              "signature": "(self) -> List[str]"
            },
            {
              "name": "run",
              "docstring": "Execute file operations.",
              "signature": "(self, action: str, filename: str = '', content: str = '') -> Dict[str, Any]"
            },
            {
              "name": "validate_and_run",
              "docstring": "Validate parameters then run the tool.",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            }
          ],
          "init_signature": "(self, base_dir: str = 'sandbox')"
        },
        {
          "name": "MemorizeTool",
          "docstring": "Tool for storing content in agent memory.",
          "module": "cogency.tools.memory",
          "methods": [
            {
              "name": "get_schema",
              "docstring": "Return tool call schema for LLM formatting.\n\nReturns:\n    String representation of the tool's parameter schema",
              "signature": "(self) -> str"
            },
            {
              "name": "get_usage_examples",
              "docstring": "Return example tool calls.",
              "signature": "(self) -> List[str]"
            },
            {
              "name": "run",
              "docstring": "Store content in memory with smart auto-tagging.",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            },
            {
              "name": "validate_and_run",
              "docstring": "Validate parameters then run the tool.",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            }
          ],
          "init_signature": "(self, memory: cogency.memory.base.BaseMemory)"
        },
        {
          "name": "RecallTool",
          "docstring": "Tool for retrieving content from agent memory.",
          "module": "cogency.tools.memory",
          "methods": [
            {
              "name": "get_schema",
              "docstring": "Return tool call schema for LLM formatting.\n\nReturns:\n    String representation of the tool's parameter schema",
              "signature": "(self) -> str"
            },
            {
              "name": "get_usage_examples",
              "docstring": "Return example tool calls.",
              "signature": "(self) -> List[str]"
            },
            {
              "name": "run",
              "docstring": "Retrieve content from memory.\n\nExpected kwargs:\n    query (str): Search query\n    limit (int, optional): Maximum number of results\n    tags (List[str], optional): Filter by tags",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            },
            {
              "name": "validate_and_run",
              "docstring": "Validate parameters then run the tool.",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            }
          ],
          "init_signature": "(self, memory: cogency.memory.base.BaseMemory)"
        },
        {
          "name": "TimezoneTool",
          "docstring": "Get current time for any timezone/city using pytz - reliable local computation.",
          "module": "cogency.tools.timezone",
          "methods": [
            {
              "name": "get_schema",
              "docstring": "Return the tool call schema.",
              "signature": "(self) -> str"
            },
            {
              "name": "get_usage_examples",
              "docstring": "Return example usage patterns.",
              "signature": "(self) -> List[str]"
            },
            {
              "name": "run",
              "docstring": "Get current time for a location.\n\nArgs:\n    location: City name or timezone (e.g., \"New York\", \"America/New_York\", \"Europe/London\")\n    \nReturns:\n    Time data including local time, timezone, UTC offset",
              "signature": "(self, location: str) -> Dict[str, Any]"
            },
            {
              "name": "validate_and_run",
              "docstring": "Validate parameters then run the tool.",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            }
          ],
          "init_signature": "(self)"
        },
        {
          "name": "WeatherTool",
          "docstring": "Get current weather for any city using wttr.in (no API key required).",
          "module": "cogency.tools.weather",
          "methods": [
            {
              "name": "get_schema",
              "docstring": "Return the tool call schema.",
              "signature": "(self) -> str"
            },
            {
              "name": "get_usage_examples",
              "docstring": "Return example usage patterns.",
              "signature": "(self) -> List[str]"
            },
            {
              "name": "run",
              "docstring": "Get weather for a city.\n\nArgs:\n    city: City name (e.g., \"San Francisco\", \"London\", \"Tokyo\")\n    \nReturns:\n    Weather data including temperature, conditions, humidity",
              "signature": "(self, city: str) -> Dict[str, Any]"
            },
            {
              "name": "validate_and_run",
              "docstring": "Validate parameters then run the tool.",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            }
          ],
          "init_signature": "(self)"
        },
        {
          "name": "WebSearchTool",
          "docstring": "Base class for all tools in the cogency framework.",
          "module": "cogency.tools.web_search",
          "methods": [
            {
              "name": "get_schema",
              "docstring": "Return tool call schema for LLM formatting.\n\nReturns:\n    String representation of the tool's parameter schema",
              "signature": "(self) -> str"
            },
            {
              "name": "get_usage_examples",
              "docstring": "Return example tool calls for LLM guidance.\n\nReturns:\n    List of example tool call strings",
              "signature": "(self) -> List[str]"
            },
            {
              "name": "run",
              "docstring": "",
              "signature": "(self, *args, **kwargs)"
            },
            {
              "name": "validate_and_run",
              "docstring": "Validate parameters then run the tool.",
              "signature": "(self, **kwargs: Any) -> Dict[str, Any]"
            }
          ],
          "init_signature": "(self)"
        }
      ],
      "functions": [
        {
          "name": "_discover_tools",
          "docstring": "Auto-discover only standalone tool classes.",
          "module": "cogency.tools",
          "signature": "()"
        },
        {
          "name": "get_tool_by_name",
          "docstring": "Get tool class by name.",
          "module": "cogency.tools",
          "signature": "(name: str)"
        },
        {
          "name": "list_available_tools",
          "docstring": "List all available tool names.",
          "module": "cogency.tools",
          "signature": "()"
        }
      ]
    },
    "nodes": {
      "name": "nodes",
      "docstring": "",
      "classes": [],
      "functions": [
        {
          "name": "memorize",
          "docstring": "Memorize content if it meets certain criteria.",
          "module": "cogency.nodes.memory",
          "signature": "(state: cogency.types.State, *, memory: cogency.memory.base.BaseMemory) -> cogency.types.State"
        },
        {
          "name": "react_loop_node",
          "docstring": "ReAct Loop Node: Full multi-step reason → act → observe cycle until task complete.",
          "module": "cogency.nodes.react_loop",
          "signature": "(state: cogency.types.State, llm: cogency.llm.base.BaseLLM, tools: Optional[List[cogency.tools.base.BaseTool]] = None, prompt_fragments: Optional[Dict[str, str]] = None, config: Optional[Dict] = None) -> cogency.types.State"
        },
        {
          "name": "select_tools",
          "docstring": "Intelligently select a subset of tools based on the user query.",
          "module": "cogency.nodes.select_tools",
          "signature": "(state: cogency.types.State, llm: cogency.llm.base.BaseLLM, tools: Optional[List[cogency.tools.base.BaseTool]] = None) -> cogency.types.State"
        }
      ]
    },
    "memory": {
      "name": "memory",
      "docstring": "Memory primitives for Cogency agents.",
      "classes": [
        {
          "name": "BaseMemory",
          "docstring": "Abstract base class for memory backends.",
          "module": "cogency.memory.base",
          "methods": [
            {
              "name": "clear",
              "docstring": "Clear all artifacts from memory.",
              "signature": "(self) -> None"
            },
            {
              "name": "forget",
              "docstring": "Remove an artifact from memory.",
              "signature": "(self, artifact_id: uuid.UUID) -> bool"
            },
            {
              "name": "inspect",
              "docstring": "Dev tooling - inspect memory state.",
              "signature": "(self) -> Dict[str, Any]"
            },
            {
              "name": "memorize",
              "docstring": "Store new content in memory.",
              "signature": "(self, content: str, memory_type: cogency.memory.base.MemoryType = <MemoryType.FACT: 'fact'>, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, timeout_seconds: float = 10.0) -> cogency.memory.base.MemoryArtifact"
            },
            {
              "name": "recall",
              "docstring": "Retrieve relevant content from memory.",
              "signature": "(self, query: str, limit: Optional[int] = None, tags: Optional[List[str]] = None, memory_type: Optional[cogency.memory.base.MemoryType] = None, since: Optional[str] = None, **kwargs) -> List[cogency.memory.base.MemoryArtifact]"
            }
          ],
          "init_signature": "(self, /, *args, **kwargs)"
        },
        {
          "name": "FSMemory",
          "docstring": "Filesystem-based memory backend.\n\nStores memory artifacts as JSON files in a directory structure.\nUses simple text matching for recall operations.",
          "module": "cogency.memory.filesystem",
          "methods": [
            {
              "name": "clear",
              "docstring": "Remove all artifact files.",
              "signature": "(self) -> None"
            },
            {
              "name": "forget",
              "docstring": "Remove artifact file.",
              "signature": "(self, artifact_id: uuid.UUID) -> bool"
            },
            {
              "name": "inspect",
              "docstring": "Dev tooling - inspect memory state.",
              "signature": "(self) -> Dict[str, Any]"
            },
            {
              "name": "memorize",
              "docstring": "Store content as JSON file.",
              "signature": "(self, content: str, memory_type: cogency.memory.base.MemoryType = <MemoryType.FACT: 'fact'>, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, timeout_seconds: float = 10.0) -> cogency.memory.base.MemoryArtifact"
            },
            {
              "name": "recall",
              "docstring": "Search artifacts with enhanced relevance scoring and async optimization.",
              "signature": "(self, query: str, limit: Optional[int] = None, tags: Optional[List[str]] = None, memory_type: Optional[cogency.memory.base.MemoryType] = None, since: Optional[str] = None, **kwargs) -> List[cogency.memory.base.MemoryArtifact]"
            },
            {
              "name": "should_store",
              "docstring": "Smart auto-storage heuristics - NO BULLSHIT.",
              "signature": "(self, text: str) -> Tuple[bool, str]"
            }
          ],
          "init_signature": "(self, memory_dir: str = '.memory')"
        },
        {
          "name": "MemoryArtifact",
          "docstring": "A memory artifact with content and metadata.",
          "module": "cogency.memory.base",
          "methods": [
            {
              "name": "decay_score",
              "docstring": "Calculate decay based on recency and confidence.",
              "signature": "(self) -> float"
            }
          ],
          "init_signature": "(self, content: str, memory_type: cogency.memory.base.MemoryType = <MemoryType.FACT: 'fact'>, tags: List[str] = <factory>, metadata: Dict[str, Any] = <factory>, id: uuid.UUID = <factory>, created_at: datetime.datetime = <factory>, relevance_score: float = 0.0, confidence_score: float = 1.0, access_count: int = 0, last_accessed: datetime.datetime = <factory>) -> None"
        },
        {
          "name": "MemoryType",
          "docstring": "Types of memory for different agent use cases.",
          "module": "cogency.memory.base",
          "methods": [],
          "init_signature": "(self, *args, **kwds)"
        }
      ],
      "functions": []
    },
    "context": {
      "name": "context",
      "docstring": "",
      "classes": [
        {
          "name": "Context",
          "docstring": "Agent operational context.",
          "module": "cogency.context",
          "methods": [
            {
              "name": "add_message",
              "docstring": "Add message to history with optional trace linkage.",
              "signature": "(self, role: str, content: str, trace_id: Optional[str] = None)"
            },
            {
              "name": "add_tool_result",
              "docstring": "Add tool execution result to history.",
              "signature": "(self, tool_name: str, args: dict, output: dict)"
            },
            {
              "name": "get_clean_conversation",
              "docstring": "Returns conversation without execution trace data and internal JSON.",
              "signature": "(self) -> List[Dict[str, str]]"
            }
          ],
          "init_signature": "(self, current_input: str, messages: List[Dict[str, str]] = None, tool_results: Optional[List[Dict[str, Any]]] = None, max_history: Optional[int] = None)"
        }
      ],
      "functions": []
    },
    "config": {
      "name": "config",
      "docstring": "Centralized configuration for Cogency.",
      "classes": [],
      "functions": [
        {
          "name": "get_api_keys",
          "docstring": "Get API keys for a given provider from environment variables.",
          "module": "cogency.config",
          "signature": "(provider: str) -> list"
        }
      ]
    },
    "types": {
      "name": "types",
      "docstring": "",
      "classes": [
        {
          "name": "State",
          "docstring": "dict() -> new empty dictionary\ndict(mapping) -> new dictionary initialized from a mapping object's\n    (key, value) pairs\ndict(iterable) -> new dictionary initialized as if via:\n    d = {}\n    for k, v in iterable:\n        d[k] = v\ndict(**kwargs) -> new dictionary initialized with the name=value pairs\n    in the keyword argument list.  For example:  dict(one=1, two=2)",
          "module": "cogency.types",
          "methods": [],
          "init_signature": "(self, /, *args, **kwargs)"
        },
        {
          "name": "Context",
          "docstring": "Agent operational context.",
          "module": "cogency.context",
          "methods": [
            {
              "name": "add_message",
              "docstring": "Add message to history with optional trace linkage.",
              "signature": "(self, role: str, content: str, trace_id: Optional[str] = None)"
            },
            {
              "name": "add_tool_result",
              "docstring": "Add tool execution result to history.",
              "signature": "(self, tool_name: str, args: dict, output: dict)"
            },
            {
              "name": "get_clean_conversation",
              "docstring": "Returns conversation without execution trace data and internal JSON.",
              "signature": "(self) -> List[Dict[str, str]]"
            }
          ],
          "init_signature": "(self, current_input: str, messages: List[Dict[str, str]] = None, tool_results: Optional[List[Dict[str, Any]]] = None, max_history: Optional[int] = None)"
        },
        {
          "name": "ExecutionTrace",
          "docstring": "Lean trace engine - just stores entries with serialization safety.",
          "module": "cogency.types",
          "methods": [
            {
              "name": "add",
              "docstring": "",
              "signature": "(self, node: str, message: str, data: dict = None, explanation: str = None)"
            }
          ],
          "init_signature": "(self)"
        },
        {
          "name": "ReasoningDecision",
          "docstring": "Structured decision from reasoning - NO JSON CEREMONY.",
          "module": "cogency.types",
          "methods": [],
          "init_signature": "(self, should_respond: bool, response_text: Optional[str] = None, tool_calls: Optional[List[Dict[str, Any]]] = None, task_complete: bool = False) -> None"
        }
      ],
      "functions": [
        {
          "name": "format_full_debug",
          "docstring": "Format full debug trace (dev mode).",
          "module": "cogency.types",
          "signature": "(trace: cogency.types.ExecutionTrace) -> str"
        },
        {
          "name": "format_trace",
          "docstring": "Format full trace with icons.",
          "module": "cogency.types",
          "signature": "(trace: cogency.types.ExecutionTrace) -> str"
        },
        {
          "name": "summarize_trace",
          "docstring": "Generate clean summary from trace entries.",
          "module": "cogency.types",
          "signature": "(trace: cogency.types.ExecutionTrace) -> str"
        }
      ]
    }
  }
}